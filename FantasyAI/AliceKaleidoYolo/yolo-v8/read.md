>
>单目3D目标检测旨在定位输入的单个2D图像中的3D边界框。这是一个极具挑战性的问题，并且仍然是开放的，特别是当没有额外的信息（例如，深度、激光雷达或多帧图像）可以用于训练和/或推断时。
>
>本文提出了一种简单而有效的单目3D目标检测方法，无需利用任何额外信息。它提出了学习单目上下文的MonoCon方法，作为训练中的辅助任务，以帮助单目3D目标检测。关键思想是使用图像中带3D边界框标注的目标，在训练中有一组丰富的良好姿势的投影2D监督信号可用，例如投影的角关键点及其相对于2D边界框中心的相关偏移向量，这些都应该被用作训练中的辅助。
>
>提出的MonoCon是由测量理论中的$Cramer–Wold$定理在高水平上推动的。在实现中，它利用一个非常简单的端到端设计来证明学习辅助单目上下文的有效性，它由三个组件组成：
>- 基于深度神经网络的特征提取网络主干
>- 用于学习3D边界框预测中使用的基本参数的多个回归头分支
>- 基于深度神经网并用于学习辅助上下文的多个回归头分支
>
>在训练之后，丢弃辅助上下文回归分支以获得更好的推理效率。
>
>在实验中，提出的MonoCon在KITTI基准（汽车、行人和自行车）中进行了测试。在汽车类别排行榜上，它超越了所有现有技术，并在准确性方面获得了行人和自行车运动员的可比性能。由于设计简单，所提出的MonoCon方法以38.7fps的速度获得了最快的推理速度。


## 简介
3D目标检测是许多计算机视觉应用的关键组成部分，例如自动驾驶和机器人导航。高性能方法通常需要更昂贵的系统配置，例如用于精确深度测量的激光雷达传感器或用于立体深度估计的立体相机，并且通常计算成本更高。

为了减轻这些“负担”，并且由于降低成本和增加模块冗余的潜在前景，单目3D目标检测旨在从输入的2D图像中定位3D目标边界框，已成为一种有前途的替代方法，在计算机视觉和人工智能界受到了广泛关注。

除了实践中的潜在优势外，开发强大的弹幕3D目标检测系统将有助于解决计算机视觉中的一个基本问题，即是否可以仅从最初丢失了深度信息的2D图像中恢复3D结构。

本文研究的是自动驾驶应用中的3D目标检测。目标是估计每个目标实例（例如2D图像中的汽车）的3D边界框。在KITTI基准中，3D边界框通过以下参数进行参数化：
1. 相机3D坐标中的3D中心位置$(x,y,z)$（以米为单位），
2. 基于从相机中心到3D目标中心的矢量的目标相对于相机的观察角度$\alpha$
3. 形状尺寸$(h,w,l)$，即高度、宽度和长度（以米为单位）。

基于MonoDLE方法在KITTI基准中所做的广泛而深入的分析，提高单目3D目标检测整体性能的一个主要挑战在于高精度推理3D中心位置。

为了解决这一挑战，在最先进的单目3D目标检测中有两种主要类型的设置，这取决于是否有额外的信息（激光雷达深度测量、通过单独训练的模型或多帧的单眼深度估计结果）用于训练和/或推断。

在实践中，3D中心位置$(x,y,z)$通常被分解为图像平面中的投影3D中心$(x_c,y_c)$和目标深度z。假设相机固有矩阵在训练和推断中都是已知的，则可以利用推断出的投影3D中心和目标深度来恢复3D位置。

![](https://files.mdnice.com/user/33808/e4088ec6-4470-407f-847c-653e8cc5a615.png)

本文专注于端到端单目3D目标检测，而不利用任何额外信息。它采用了CenterNet中提出的Anchor偏移公式，基于2D边界框中心（即Anchor）学习投影的3D中心，并提出了一种简单而有效的方法，有助于更好的整体性能（图1）。关键思想是在训练中利用单目上下文作为辅助学习任务，以提高性能（图2）。

基本原理是，利用图像中目标的带注释的3D边界框，在训练中有一组丰富的良好姿势的投影2D监督信号，例如投影的角关键点及其相对于Anchor的相关偏移向量。它们应该在训练中被利用，以引导单目3D目标检测的更具表现力的表示。因此，提出的方法被称为MonoCon。

从统计学上讲，单目上下文可以被视为图像平面中的边缘随机变量，这些边缘随机变量是从3D边界框随机变量投影出来的。在测度理论中，`Cramer-Wold`定理指出，$R^k$上的Borel概率测度由其一维投影的总体唯一确定。

受`Cramer-Wold`定理的启发，提出的MonoCon方法在训练中引入单目投影作为辅助任务，以学习单目3D目标检测的更有效表示。同时，它寻求整体检测系统的最低限度的简单设计，以证明基本原理和高级动机的有效性。

![](https://files.mdnice.com/user/33808/271c8bfd-26b0-47a1-8b47-b9a0d2a816fb.png)

在实现中，所提出的MonoCon利用了由3个组件组成的非常简单的设计（图2）：
1. 基于深度神经网络（DNN）的特征主干
2. 用于学习3D边界框预测中使用的基本参数的多个回归头分支
3. 用于学习辅助上下文的多个回归头分支，并在推理过程中丢弃辅助上下文回归分支

在实验中，提出的MonoCon在KITTI基准（汽车、行人和自行车）中进行了测试。在汽车类别排行榜上，它优于现有技术（包括使用激光雷达、深度或多帧额外信息的方法），并在准确性方面获得了行人和自行车运动员的可比性能。得益于简单的设计，MonoCon以38.7fps（在单个NVIDIA 2080Ti GPU卡上）获得了最快的速度。

>补充知识：
> **Cramer分解定理**：任何一个时间序列都可以分解为确定性趋势成分和平稳的零均值误差成分。
>
>![](https://files.mdnice.com/user/33808/d1e6541d-ee1b-43a9-a61c-88e938292c80.png)
> 其中$d < \infty,β_1,β_2,…,β_d $,为常数系数，${a_t}$为一个零均值白噪声序列; B为延迟算子。
>
>Cramer分解说明任何一序列的波动都可以视为受到了确定性影响和随机性影响的综合作用。
>

## 2、相关工作与本文贡献
### 2.1、辅助任务和辅助学习
机器学习辅助任务是指在训练中利用的任务，其唯一目的是更好地辅助推理中使用的任务。因此，学习过程被称为辅助学习，而不是多任务学习，因为对于多任务学习来说，训练中的所有任务都会在推理过程中使用。

辅助任务和辅助学习在广泛的领域显示出许多成功的应用，包括计算机视觉、自然语言处理和强化学习。尽管简单，但在单目3D目标检测中，开发全面的2D辅助任务尚未得到很好的研究。提出的MonoCon在KITTI基准中展示了辅助学习在性能（汽车类别）和推理效率方面的优势。

### 2.2、具有额外信息的单目3D检测
由于单目3D的不适定性，单目3D检测明显落后于基于激光雷达和基于立体图像的对应工作。因此，许多单目3D检测方法借助额外信息寻求解决方案，例如激光雷达数据、离线单目深度估计模块（使用密集深度图进行预处理）、多帧，或CAD模型等。

尽管这些方法显示了有希望的结果，但大多数这些模型严重依赖额外的模块（即深度估计模块等），这需要额外的计算成本。因此，这些方法的推理速度通常很慢（低于10fps），严重阻碍了它们在实时自动驾驶中的应用。

所提出的MonoCon方法不使用任何额外的信息，并且寻求一种具有高性能和实时推理速度的简单设计。一个动机是在利用使用多视图图像或使用更多传感器的配置之前，了解纯单目3D检测方法的“真正的”极限。

### 2.3、没有额外信息的单目3D检测
自从Deep3DBox的开创性工作以来，人们提出了许多努力，以利用2D-3D几何约束来提高3D检测性能，这通常被视为多任务学习，而非辅助学习。

例如，在RTM3D方法中，所有学习到的2D任务都用作优化项，以在推理中的后处理（使用PnP方法）中计算3D位置。在MonoRCNN方法中，使用了一个2D任务（即2D box）。2D box预测用于计算深度以及推理中的3D box大小。

SMOKE方法不学习任何2D任务。SMOKE的一个主要观点是2D任务会干扰3D任务的学习。在利用2D-3D几何约束时，现有工作明确基于2D预测来计算3D位置，因此经常受到误差放大效应的影响。也因此，最近的工作尝试使用不确定性建模（例如，GUPNet）或复杂的模型集成（例如，MonoFlex）。

提出的MonoCon的目标是研究2D辅助任务在训练中的影响，并消除推理中的潜在误差放大效应，从而提高性能。最近，人们正在努力探索如何生成外变量或距离变量表示以提高3D检测性能。

所提出的MonoCon是对上述方法的补充，通过利用从3D边界框投影的适定2D上下文作为辅助学习任务。它具有使用上述方法容易扩展的潜力，性能进一步提高。

### 2.4、本文的贡献
本文对单目3D目标检测做出了以下3个主要贡献：
1. 它提出了一种简单但令人惊讶的有效方法，MonoCon，用于通过学习辅助单目上下文进行纯单目3D目标检测。在高层次上，所提出的MonoCon公式可以用测量理论中的`Cramer-Wold`定理（Cramer和Wold 1936）来解释。
2. 它在KITTI 3D目标检测基准中显示了汽车类别的最先进性能，大大优于现有技术。它在行人和自行车类别上获得了可比的性能。它可以以38.7fps的速度运行，比现有技术更快。
3. 通过探索和利用超越自动驾驶（例如，机器人导航）的一般应用中的更多辅助环境，它为开发更强大和高效的单目3D目标检测系统提供了方向。

## 3、本文方法
### 3.1 方法定义
设$Λ$为图像格（例如KITTI基准中的384×1280），$I_Λ$为定义在格上的图像。如前所述，单目3D目标检测的目的是预测$I_Λ$中每个目标实例的标签（如汽车、行人和骑自行车者）和3D边界框。

3D边界框由3D中心位置$(x,y,z)$、形状尺寸$(h,w,l)$和观测角度$\alpha∈[−\pi,\pi]$组成并进行参数化表示，所有这些都是在摄像机坐标系中测量出来的。由于观测角度与图像外观的潜在关系更强，因此它被用于预测。这里假设摄像机的固有矩阵在训练和推理中都是已知的。

### 3.2 困难
通常，使用由诸如卷积神经网络（CNN）的Backbone来提取特征，直接回归形状尺寸和方向。直接回归方法也显示了它们各自的良好性能。

同时，总体3D边界框预测性能（例如，基于IOU的平均精度（AP））对形状尺寸和方向相对较不敏感，这意味着如果能够以高精度预测3D中心位置，即使没有准确预测形状尺寸和方向，AP也不会显著降低。

相比之下，即使对形状尺寸和方向进行了非常精确的估计，如果3D中心位置受到干扰，AP也会下降。根本原因是形状尺寸（大约在1米到3米之间）和3D位置（大约1米到60米之间）之间的显著差距，并且在单目图像中测量到的这两个尺寸的不确定性将对整体AP产生显著不同的影响。

在学习投影的3D中心$(x_c,y_c)$时有2种不同的公式：
- 第一种是**通过学习热力图表示来直接预测它**，对于落在图像平面之外的投影中心要么在训练中被简单地丢弃，要么在图像边缘和从2D边界框的中心到外部投影的3D中心的线之间的交点的帮助下被处理;

- 第二种是**根据CenterNet将投影的3D中心进一步分解为2D边界框中心$(x_b,y_b)$（即图像平面内的Anchor）和偏移/位移矢量$(∆x,∆y)$**，其中$x_c=x_b+∆x$和$y_c=y_b+∆y$。

>由于偏移向量的变化很大，很难学习它们。因此，后者在整体性能方面通常不如前者，尽管它是用于学习投影的3D中心的直观且通用的表示。所提出的方法表明，当在训练中利用足够的单目上下文信息辅助时，后者可以很好地工作。

### 3.3 本文方法MonoCon
![](https://files.mdnice.com/user/33808/271c8bfd-26b0-47a1-8b47-b9a0d2a816fb.png)

#### 1、Backbone
给定一个3×H×W维度的输入RGB图像$I_Λ$，利用特征主干提取的特征图维度为D×h×w，

![](https://files.mdnice.com/user/33808/f724441d-82ca-4505-ade6-aa9274a61a43.png)

其中$Θ$收集所有可学习参数，D是输出特征图维度（例如，D=512），h和w由主干中的总体stride/sub-sampling rate s决定。

这里使用DLA网络作为Backbone，该网络被广泛用于单目3D目标检测，以便在实验中进行公平比较。

#### 2、3D边界框回归头
本文采用Anchor偏移公式来学习图像平面中投影的3D边界框中心$(x_c,y_c)$。

回归头用于计算每个c类的2D边界框中心$(x_b,y_b)$的类别热力图$H^b$（例如，c＝3表示KITTI基准中的汽车、行人和自行车），

![](https://files.mdnice.com/user/33808/8ce5a63d-068f-449f-81b1-f97af78a3f1e.png)

其中，$g(·;\Phi^b)$由一个具有可学习参数$\Phi^b$的轻量级模块实现，

![](https://files.mdnice.com/user/33808/186c07ed-ada8-4b71-a37e-ffde7d2ebe34.png)

其中第1卷积还将特征维度降低为d（例如，d=64)为轻量级，AN表示注意力归一化（AN），这是一个轻量级模块集成特征归一化和通道级特征注意力（例如SE模块)。

由于其对标准化后的仿射变换重新校准特征的混合建模公式，在回归头中采用其学习更具表现性的潜在特征表示$F_{d×h×w}$。

轻量级的模块体系结构（等式3）被所有具有不同实例化（即不同的可学习参数）的回归头所使用。

从2D边界框中心$(x_b,y_b)$到投影过后的3D边界框中心$(x_c,y_c)$的偏移向量为：${(\Delta x^c_b,\Delta x^c_b)}$，其回归头定义为：

![](https://files.mdnice.com/user/33808/c3f04fe0-3ff9-4bfb-9103-bd6e0fa6448d.png)

同样，深度和形状尺寸分别对应的回归头如下：

![](https://files.mdnice.com/user/33808/b531a6b5-62c8-4f35-9914-01a3c50e6b44.png)

其中：
- $g(F;Θ^Z)$估计深度及其不确定性；
- inverse sigmoid变换应用于处理$g(F;Θ^Z)$的无界输出，如CenterNet中所做的，并且是一个小的正常数，以确保数值稳定性；
- $\sigma^Z$用于建模深度估计中的异方差随机不确定性，如MonoEF所述。

对于观测角度$\alpha$，使用Deep3DBox提出的 multi-bin setting。角度范围$[-π,π]$被均匀地划分为预定义数量的b个非重叠bin（例如，b=12）。观察角回归头如下式：

![](https://files.mdnice.com/user/33808/51a512ed-93ab-43cc-a540-a07cebf29fc6.png)

其中，通过计算第1个b通道的bin指数$\alpha_i\in \{0,1,...,11\}$（使用沿着b通道的softmax之后的arg max）和A的第2个b通道中的相应角度残差$\alpha_r$，以及确保$\alpha\in [-π,π]$的适当转换来预测观测角$\alpha$。

#### 3、计算预测的3D边界框
基于非最大抑制（NMS）和阈值$\tau$（例如，$\tau$=0.2）阈值化后热力图$H^b$（方程2）的每个通道中的峰值，为每个类别检测一组2D边界框的中心。

在不失一般性的情况下，考虑检测到的汽车2D边界框中心$(x_b,y_b)$，从$O^c$（方程4）中检索偏移矢量，$(∆x_b,∆y_b)=O^c(x_b,y_b)$。然后，通过$(x_c,y_c)=(x_b+∆x_b,y_b+∆y_b)$预测汽车投影后的3D中心。通过$z＝z(x_b,y_b)$预测相应的深度。使用相机固有矩阵，将以简单的方式计算3D位置$(x,y,z)$。

类似地，可以预测汽车的形状尺寸$(h,w,l)$和观察角度$\alpha$。预测出所有参数后，将可以得到预测的3D边界框。

#### 4、辅助上下文回归头
本文提出的MonoCon方法利用来自3D边界框的4种类型的投影信息作为辅助学习任务。

#### 4.1、投影关键点的热力图
如在计算2D边界框中心热力图$H^b$（等式2）中所做的，第一类辅助上下文是由投影的8个角点和3D边界框的投影中心组成的9个投影关键点的热力图，

![](https://files.mdnice.com/user/33808/545ec346-11c9-40a4-8ecd-26524d8d79a2.png)

####  4.2、8个投影角点的偏移矢量
除了从2D边界框中心到投影的3D边界框中心的偏移向量$O^c$（等式4）之外，第2类辅助上下文是从2D边界框中心到3D边界框的8个投影角点的偏移向量，

![](https://files.mdnice.com/user/33808/b6dc2bfc-f09d-4f7d-829f-f6b0f20f3bff.png)

注意，这与方程4结合起来，其中在实现中共享$g(·)$中的第1卷积块。

#### 4.3、2D边界框大小
这与CenterNet的做法相同。直接回归2D边界框的高度和宽度，

![](https://files.mdnice.com/user/33808/372c5bff-44fc-42b2-afe2-366fcb7714db.png)

#### 4.4、关键点位置的量化残差
由于特征主干中的整体stride s（通常是s>1），在将stride s相乘后，原始输入图像$I^Λ$中的像素位置与其输出特征图F中对应的像素位置之间存在量化残差。

考虑原始图像中汽车的2D边界框中心$(x^∗_b,y_b^∗)$，其在特征图F中的像素位置为$(x_b=\lfloor x^∗_b/s \rfloor，y_b=\lfloor y^∗_b/s\rfloor)$，量化残差定义为：

![](https://files.mdnice.com/user/33808/ff76d036-cb19-4daf-8087-a633dcaf38ba.png)

这里分别对2D边界框中心$(x_b,y_b)$和9个投影关键点$(x_k,y_k)$的残差进行建模，以说明这些点的本质的潜在差异。

后者以关键点不可知的方式建模，如图2所示有，

![](https://files.mdnice.com/user/33808/21be51be-3189-45b8-b184-9ce926b14937.png)

#### 5、损失函数
使用了5种广泛用于单目3D目标检测的损失函数，包括：
1. 用于CenterNet中使用的热力图（等式2和等式9）的高斯核加权Focal Loss函数；
2. 用于深度估计的拉普拉斯不确定性损失函数（等式5和等式6）；
3. 用于形状维度的维度感知的L1损失函数（等式7）；
4. 用于观察角度中的bin指数的标准交叉熵损失函数（等式8）；
5. 偏移向量的标准L1损失函数，（等式4和等式10），观察角度中bin内角残差（等式8），2D边界框大小（等式11）和量化残差（等式13和等式14）。

简要讨论前3个问题如下：

#### 热力图的高斯核加权Focal Loss
在不丧失一般性的情况下，考虑回归热力图H1×h×w（例如，汽车的2D边界框中心），GT热力图h*1×h×w也以回归热力图的分辨率生成。

对于原始图像中的每个GT中心点$(x^*_b,y_b^*)∈P$，其在GT热力图中的位置为$(x_b=\lfloor x^∗_b/s \rfloor，y_b=\lfloor y^∗_b/s\rfloor)$（其中s是特征主干的整体stride）。

高斯核$G(x,y)=exp(−\frac{(x-x_b)^2+(y-y_b)^2}{2\cdot\sigma^2_b})$用于建模中心点，其中$\sigma_b$是预定义的目标大小自适应标准偏差。如果2个高斯核重叠，则保持元素最大值。然后，所有的$G(·,·)$被坍缩，形成GT热力图$H^*$。

损失函数由，

![](https://files.mdnice.com/user/33808/fa655d83-f7f2-4063-85a0-cebec068c219.png)

其中$N=|P|$是GT点的数量。β和γ是超参数（例如β=4.0和γ=2.0）。

#### Laplacian任意深度不确定性损失函数
用$Z_1^*×h×w$表示GT（稀疏）深度图，其中注释的3D边界框的GT深度被分配给h×w格中的相应GT 2D边界框中心位置，即$Z^*(x_b,y_b)$（采用与方程5中相同的 inverse sigmoid 变换）。

拉普拉斯分布用于建模不确定性$\sigma^Z$（等式6）。对于预测深度Z（等式5），

![](https://files.mdnice.com/user/33808/b6f1860d-223f-4ca1-b658-d29ca47bad39.png)

其中，P是GT 2D边界框中心点的集合，$\sigma_b^Z=\sigma^Z(x_b,y_b)$，$z_b=Z(x_b,y_b)$,$z_b^*=Z^*(x_b,y_b)$。

#### 形状尺寸的维度感知L1损失函数
它是由基于Oriented IOU优化推动的，实现了标准L1损失的重新分布。类似地，假设$S^{3D^*}$是分配给h×w网格中的GT 2D边界框中心位置的形状维度映射，对于预测的形状维度$S^{3D}$（等式7），损失函数由下式定义：，

![](https://files.mdnice.com/user/33808/c3725cd1-5ac3-42e9-827f-68d7a037540a.png)

其中λ是补偿权重，以确保尺寸感知L1损失具有与标准L1损失相同的值，根据定义，标准L1损失与应用补偿权重之前的尺寸损失之间的比率（无训练中的梯度）。

#### 总损失
只是所有损失项的总和，每个损失项都有一个权衡权重参数。为了简单起见，除了使用0.1的2D尺寸L1损失外，对所有损失项使用1.0。

## 4、实验
### 4.1、消融实验
#### 1、辅助上下文的学习和注意力规范化（AN）的重要性
表3的比较，表明了所提出的MonoCon的有效性，并证明了设计选择的重要性。

一方面，如果没有辅助组件，MonoCon类似于MonoDLE方法。

![](https://files.mdnice.com/user/33808/e10ed6fd-3a91-4910-91c6-4614f324c22e.png)

作者重新训练了一个增强的MonoDLE模型，它获得了比标准MonoDLE方法的明显优秀的性能。MonoCon的性能仍然大大优于增强的MonoDLE。

另一方面，测试了从(a)到(g)的MonoCon模型的7个变体。辅助上下文明显比注意力规范化更重要： 
- (g)与(a)相比，在中等难度设置下，缓解增加了8.49%，这清楚地显示了建议的公式对某些实现调优的重要性；
- 从(b)到(f)，根据没有它们训练的模型的性能对辅助上下文的重要性进行排序：性能越低，上下文(s)就越重要。

#### 2、类别不可知论设置对回归头和训练设置的影响
![](https://files.mdnice.com/user/33808/02db2f6b-7233-4f18-8a22-028f1c3076a3.png)

表4显示了这些比较情况。
- 一方面，使用类别不可知的设计对汽车和骑自行车的类别表现出更好的表现，而特定类别的设计对行人类别明显更好；

- 另一方面，联合训练这3类是有益的，这表明可能存在一些类别间的协同作用。

### 4.2、SOTA对比
![](https://files.mdnice.com/user/33808/c4af8616-987f-4bcb-88a2-c99529d3c330.png)


![](https://files.mdnice.com/user/33808/e6a1fb4a-395c-4b91-9758-06d79a9af6cc.png)

## 5、总结
本文提出了一种简单而有效的不利用任何额外信息的单目3D目标检测公式。它提出了MonoCon方法，它学习在训练中从3D边界框投射的辅助单目上下文。

所提出的MonoCon在实现中采用了一个简单的设计，包括一个ConvNet特征主干和一个具有相同模块架构的回归头列表，用于基本参数和辅助上下文的回归。

在实验中，所提出的MonoCon在KITTI 3D目标检测基准中进行了测试，在汽车类别上具有最先进的性能，在行人和自行车类别上具有可比的性能。

## 6、参考
[1].Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection.<br>



